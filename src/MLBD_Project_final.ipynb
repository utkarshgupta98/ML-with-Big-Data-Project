{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Module 1: Distributed Data Ingestion & Exploration"
      ],
      "metadata": {
        "id": "WPdt7AeRXdLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install and Set Up PySpark"
      ],
      "metadata": {
        "id": "TsnCFn0oXiA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mQSw3ZpkXbOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111348ab-dade-446f-fa9b-478de3b4bae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import Required Libraries"
      ],
      "metadata": {
        "id": "W0uU_ev7XksZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, length, when, isnan, count\n"
      ],
      "metadata": {
        "id": "By2xt8AgXjYN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Initialize Spark Session"
      ],
      "metadata": {
        "id": "50NwXOZWXnWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PalatePatterns_Module1\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "GMTEDF9xXl4q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Load the Amazon Fine Food Reviews Dataset"
      ],
      "metadata": {
        "id": "J0sK5lcKXrA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/content/Reviews.csv\", header=True, inferSchema=True)\n",
        "df.printSchema()\n",
        "df.show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "KDfs3n_vXo6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c373151-477a-4dc7-816d-a854443f04ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: integer (nullable = true)\n",
            " |-- ProductId: string (nullable = true)\n",
            " |-- UserId: string (nullable = true)\n",
            " |-- ProfileName: string (nullable = true)\n",
            " |-- HelpfulnessNumerator: string (nullable = true)\n",
            " |-- HelpfulnessDenominator: string (nullable = true)\n",
            " |-- Score: string (nullable = true)\n",
            " |-- Time: string (nullable = true)\n",
            " |-- Summary: string (nullable = true)\n",
            " |-- Text: string (nullable = true)\n",
            "\n",
            "+---+----------+--------------+-----------------------------------+--------------------+----------------------+-----+----------+-------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Id |ProductId |UserId        |ProfileName                        |HelpfulnessNumerator|HelpfulnessDenominator|Score|Time      |Summary                  |Text                                                                                                                                                                                                                                                                                                                                                                                              |\n",
            "+---+----------+--------------+-----------------------------------+--------------------+----------------------+-----+----------+-------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1  |B001E4KFG0|A3SGXH7AUHU8GW|delmartian                         |1                   |1                     |5    |1303862400|Good Quality Dog Food    |I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.                                                                                                                           |\n",
            "|2  |B00813GRG4|A1D87F6ZCVE5NK|dll pa                             |0                   |0                     |1    |1346976000|Not as Advertised        |\"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"\"Jumbo\"\".\"                                                                                                                                                                                                |\n",
            "|3  |B000LQOCH0|ABXLMWJIXXAIN |\"Natalia Corres \"\"Natalia Corres\"\"\"|1                   |1                     |4    |1219017600|\"\"\"Delight\"\" says it all\"|\"This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"\"The Lion|\n",
            "|4  |B000UA0QIQ|A395BORC6FGVXV|Karl                               |3                   |3                     |2    |1307923200|Cough Medicine           |If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.                                                                                                                                                                       |\n",
            "|5  |B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigham \"\"M. Wassir\"\"\"  |0                   |0                     |5    |1350777600|Great taffy              |Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.                                                                                                                                                                                                                                                      |\n",
            "+---+----------+--------------+-----------------------------------+--------------------+----------------------+-----+----------+-------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Clean Data and Handle Missing Values"
      ],
      "metadata": {
        "id": "MK6kucp9XuK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep rows where 'Text' and 'Score' are present and valid\n",
        "df_clean = df.filter((col(\"Text\").isNotNull()) & (col(\"Score\").isNotNull()))\n",
        "df_clean = df_clean.filter((col(\"Score\").cast(\"int\").between(1, 5)))\n",
        "df_clean = df_clean.withColumn(\"Score\", col(\"Score\").cast(\"int\"))\n"
      ],
      "metadata": {
        "id": "iph525vUXsVB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Simulate MapReduce: Review Count by Score"
      ],
      "metadata": {
        "id": "4el6UDblXyDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mimic MapReduce 'Reduce' step: Aggregation\n",
        "score_count = df_clean.groupBy(\"Score\").count().orderBy(\"Score\")\n",
        "score_count.show()\n"
      ],
      "metadata": {
        "id": "Nfw2SjdaXvaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5457a6-b59d-4d10-efd2-35116003a575"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "|Score| count|\n",
            "+-----+------+\n",
            "|    1| 52635|\n",
            "|    2| 29877|\n",
            "|    3| 42502|\n",
            "|    4| 80141|\n",
            "|    5|361648|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save ratings distribution\n",
        "score_count.toPandas().to_csv(\"/content/score_distribution.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "qeuv_S5V3n09"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Add Preprocessing Feature: Review Length"
      ],
      "metadata": {
        "id": "NmTreosSX2Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_clean.withColumn(\"ReviewLength\", length(col(\"Text\")))\n",
        "df_clean.select(\"Score\", \"ReviewLength\").show(5)\n"
      ],
      "metadata": {
        "id": "YLg17jsmXzRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80eb52b1-d0f7-4e24-ca47-3d3b6da0ee47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|Score|ReviewLength|\n",
            "+-----+------------+\n",
            "|    5|         263|\n",
            "|    1|         194|\n",
            "|    4|         386|\n",
            "|    2|         219|\n",
            "|    5|         140|\n",
            "+-----+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save review length stats\n",
        "review_length_stats = df_clean.groupBy(\"Score\").agg({\"ReviewLength\": \"avg\"}).toPandas()\n",
        "review_length_stats.to_csv(\"/content/review_length_stats.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "4ir8hLo43r7t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Drop Duplicate Reviews (UserId + Text)"
      ],
      "metadata": {
        "id": "A3QZqKO4X5OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_clean.dropDuplicates([\"UserId\", \"Text\"])\n"
      ],
      "metadata": {
        "id": "u7JU8xzpX3t1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Save Preprocessed Data as Partitioned Parquet (HDFS-style layout)"
      ],
      "metadata": {
        "id": "Wj3S72OJX9Hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save for downstream ML modules\n",
        "df_clean.write.mode(\"overwrite\").partitionBy(\"Score\").parquet(\"/content/module1_parquet_output\")\n"
      ],
      "metadata": {
        "id": "tTkAPJ3fX6Vx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 2: Streaming Preprocessing & Feature Engineering"
      ],
      "metadata": {
        "id": "RSHWy4hNaCFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load Data from Module 1 Output (Parquet)"
      ],
      "metadata": {
        "id": "HlTSZ0H8aHL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the preprocessed Parquet data from Module 1\n",
        "stream_df = spark.read.parquet(\"/content/module1_parquet_output\")\n",
        "stream_df.select(\"Score\", \"Text\").show(5)\n"
      ],
      "metadata": {
        "id": "Jq8c8ezoX-kT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13181eba-d867-45e4-fe4e-5ab3291989a6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|Score|                Text|\n",
            "+-----+--------------------+\n",
            "|    5|This will be the ...|\n",
            "|    5|I drank this on i...|\n",
            "|    5|Ordered 4 boxes. ...|\n",
            "|    5|What I loved abou...|\n",
            "|    5|I received a very...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Assign Sentiment Labels"
      ],
      "metadata": {
        "id": "YYV_xZkNaKHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "stream_df = stream_df.withColumn(\"Sentiment\",\n",
        "    when(col(\"Score\") >= 4, \"Positive\")\n",
        "    .when(col(\"Score\") == 3, \"Neutral\")\n",
        "    .otherwise(\"Negative\"))\n",
        "\n",
        "stream_df.groupBy(\"Sentiment\").count().show()\n"
      ],
      "metadata": {
        "id": "eVEqqsgCaIZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcd0c51-89ea-44c5-9dd3-5ffb4eb587fb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|Sentiment| count|\n",
            "+---------+------+\n",
            "| Positive|305119|\n",
            "|  Neutral| 29608|\n",
            "| Negative| 57349|\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save sentiment distribution\n",
        "stream_df.groupBy(\"Sentiment\").count().toPandas().to_csv(\"/content/sentiment_distribution.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Z70AhYGR31NF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Tokenize & Clean the Text"
      ],
      "metadata": {
        "id": "6fDZ0aDzaMxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"tokens\")\n",
        "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"clean_tokens\")\n",
        "\n",
        "tokenized_df = tokenizer.transform(stream_df)\n",
        "cleaned_df = remover.transform(tokenized_df).select(\"clean_tokens\", \"Sentiment\", \"Score\")\n",
        "cleaned_df.show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "OEoSSRpKaLSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b989f2-f72f-49fe-ba9b-3632b3a5111e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
            "|clean_tokens                                                                                                                                                                                                                                                                                                                                                                      |Sentiment|Score|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
            "|[bottle, grab, fridge, really, thirsty!, , bottles, big!, , like, grab, one, drink, couple, hours, really, stay, hydrated., , taste, cool,, clean, refreshing., , aftertaste, all., , good, water.]                                                                                                                                                                               |Positive |5    |\n",
            "|[drank, ice, workout., crisp,, cool,, refreshing!, normally, bottled, water, fan,, seem, taste, like, plastic, bottle, me., product, distinctive, sharp, clean, taste,, like, extra, filtered, something., tastes, better, filtered, water, coming, dispenser, fridge.<br, /><br, />i, buy, this!!!, delicious, refreshing!]                                                      |Positive |5    |\n",
            "|[ordered, 4, boxes., 4, different, flavors., issues, grounds, explosion, posters, 4, boxes., disappointed.<br, /><br, />brooklyn, bean, replaced, boxes, immediately., , problem, explosions., , coffee, good, too., , customer, service, top, notch.]                                                                                                                            |Positive |5    |\n",
            "|[loved, k-cup, coffee, bold, flavor., , since, doctor, told, drink, caffein, anymore, forlorned....but, realized, really, enjoy, good, cup, coffee, flavor,, caffein., , wow, coffee, tastes, wonderful., , go-to, coffee, everyday!, , highly, recommend, it!, , and,, way,, k-cup, worked, perfectly., , love, filter., , high, quality., , thanks, brooklyn, bean, roastery!!!]|Positive |5    |\n",
            "|[received, generous, sample, pack, brooklyn, bean., strong, coffee, drinker, prefer, light, roast., flavorful, new, favorite!, coffee, grain, explosions, another, reviewer, mentioned.]                                                                                                                                                                                          |Positive |5    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Simulate Streaming Batches (e.g., 30,000 rows per batch)"
      ],
      "metadata": {
        "id": "bPpXh-PMaP0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = cleaned_df.count()\n",
        "batch_size = 30000\n",
        "\n",
        "for start in range(0, total_rows, batch_size):\n",
        "    batch = cleaned_df.limit(start + batch_size).subtract(cleaned_df.limit(start))\n",
        "    print(f\"\\n Batch {start//batch_size + 1}\")\n",
        "    batch.groupBy(\"Sentiment\").count().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-SMbgUcaN-n",
        "outputId": "a9c92b7e-f163-4f4e-d8b0-cc33652d2e5a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Batch 1\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Positive|29997|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 2\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Positive|29999|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 3\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Positive|29997|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 4\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Positive|29994|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 5\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Positive|29991|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 6\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Positive|29993|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 7\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Positive|29989|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 8\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Positive|29981|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 9\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Positive|29992|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 10\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Positive|29997|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 11\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Negative|24867|\n",
            "| Positive| 5118|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 12\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "|  Neutral|20784|\n",
            "| Negative| 9211|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 13\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Negative|21171|\n",
            "|  Neutral| 8820|\n",
            "+---------+-----+\n",
            "\n",
            "\n",
            " Batch 14\n",
            "+---------+-----+\n",
            "|Sentiment|count|\n",
            "+---------+-----+\n",
            "| Negative| 2076|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Count Frequent Words Using Explode"
      ],
      "metadata": {
        "id": "XckxHVakaUHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "exploded = cleaned_df.withColumn(\"word\", explode(\"clean_tokens\"))\n",
        "exploded.groupBy(\"word\").count().orderBy(\"count\", ascending=False).show(20, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsZjKvjmaSWW",
        "outputId": "9727b0cb-ba75-4629-8b4f-20134893f661"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|word   |count |\n",
            "+-------+------+\n",
            "|       |608839|\n",
            "|like   |142952|\n",
            "|/><br  |133047|\n",
            "|good   |91473 |\n",
            "|one    |89051 |\n",
            "|great  |83573 |\n",
            "|taste  |79610 |\n",
            "|love   |75567 |\n",
            "|product|65969 |\n",
            "|get    |62824 |\n",
            "|coffee |62427 |\n",
            "|really |59348 |\n",
            "|tea    |57551 |\n",
            "|flavor |56809 |\n",
            "|use    |50220 |\n",
            "|much   |49396 |\n",
            "|little |49367 |\n",
            "|it.    |48601 |\n",
            "|buy    |45001 |\n",
            "|find   |44825 |\n",
            "+-------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Use freqItems for Frequent Word Mining"
      ],
      "metadata": {
        "id": "mgC3QQpRaW9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_items = exploded.select(\"word\").freqItems([\"word\"], support=0.02)\n",
        "freq_items.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwJ87qwOaVb1",
        "outputId": "f5a63607-7fcc-4967-adf9-a8380cd4826e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|word_freqItems                                                                                                                                                                                                                                                                                          |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[30, good, make, />not, also, tried, love, butter, , quite, cereal, hot, alot, organic, like, boxes, inside, />would, chocolate, box, adjustment, broken, <a, else, href=\"\"http://www.amazon.com/gp/product/b001eq56e4\"\">envirokidz, \"i, item?, used.<br, peanut, lemurs, 25, ordered, and,, kind, web.]|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Jaccard Similarity Between Example Reviews"
      ],
      "metadata": {
        "id": "SYP6beGdaaTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, MinHashLSH\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"clean_tokens\", outputCol=\"features\", numFeatures=1000)\n",
        "featurized_data = hashingTF.transform(cleaned_df)\n",
        "\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
        "lsh_model = mh.fit(featurized_data)\n",
        "\n",
        "# Self join to get similar pairs\n",
        "similar_pairs = lsh_model.approxSimilarityJoin(featurized_data, featurized_data, 0.6, distCol=\"JaccardDistance\")\n",
        "similar_pairs.select(\"datasetA.clean_tokens\", \"datasetB.clean_tokens\", \"JaccardDistance\").show(5, truncate=False)\n",
        "\n",
        "# Sample a smaller fraction for faster similarity join\n",
        "# small_sample = featurized_data.sample(withReplacement=False, fraction=0.01, seed=42)\n",
        "\n",
        "# Then perform similarity join on this smaller set\n",
        "# similar_pairs = lsh_model.approxSimilarityJoin(small_sample, small_sample, 0.6, distCol=\"JaccardDistance\")\n",
        "# similar_pairs.select(\"datasetA.clean_tokens\", \"datasetB.clean_tokens\", \"JaccardDistance\").show(5, truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "ocEoZfNoaYHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Save Output for Next Module"
      ],
      "metadata": {
        "id": "1CK76f6YadVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.write.mode(\"overwrite\").parquet(\"/content/streaming_preprocessed_data\")\n"
      ],
      "metadata": {
        "id": "u8DZzzrHabgk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3: Scalable Sentiment Classification"
      ],
      "metadata": {
        "id": "NwSEIUY_HQPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load Preprocessed Data"
      ],
      "metadata": {
        "id": "h1ymsJU4HSk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(\"/content/streaming_preprocessed_data\")\n",
        "df.select(\"clean_tokens\", \"Sentiment\", \"Score\").show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN_TbWkEaefG",
        "outputId": "992c8b0f-895a-4154-83ca-f4163870f964"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
            "|clean_tokens                                                                                                                                                                                                                                                                                                                                                                      |Sentiment|Score|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
            "|[bottle, grab, fridge, really, thirsty!, , bottles, big!, , like, grab, one, drink, couple, hours, really, stay, hydrated., , taste, cool,, clean, refreshing., , aftertaste, all., , good, water.]                                                                                                                                                                               |Positive |5    |\n",
            "|[drank, ice, workout., crisp,, cool,, refreshing!, normally, bottled, water, fan,, seem, taste, like, plastic, bottle, me., product, distinctive, sharp, clean, taste,, like, extra, filtered, something., tastes, better, filtered, water, coming, dispenser, fridge.<br, /><br, />i, buy, this!!!, delicious, refreshing!]                                                      |Positive |5    |\n",
            "|[ordered, 4, boxes., 4, different, flavors., issues, grounds, explosion, posters, 4, boxes., disappointed.<br, /><br, />brooklyn, bean, replaced, boxes, immediately., , problem, explosions., , coffee, good, too., , customer, service, top, notch.]                                                                                                                            |Positive |5    |\n",
            "|[loved, k-cup, coffee, bold, flavor., , since, doctor, told, drink, caffein, anymore, forlorned....but, realized, really, enjoy, good, cup, coffee, flavor,, caffein., , wow, coffee, tastes, wonderful., , go-to, coffee, everyday!, , highly, recommend, it!, , and,, way,, k-cup, worked, perfectly., , love, filter., , high, quality., , thanks, brooklyn, bean, roastery!!!]|Positive |5    |\n",
            "|[received, generous, sample, pack, brooklyn, bean., strong, coffee, drinker, prefer, light, roast., flavorful, new, favorite!, coffee, grain, explosions, another, reviewer, mentioned.]                                                                                                                                                                                          |Positive |5    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Convert Sentiment to Label"
      ],
      "metadata": {
        "id": "jUBJq_BjIAre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\")\n",
        "df = indexer.fit(df).transform(df)\n"
      ],
      "metadata": {
        "id": "4MGwSYx3HT9a"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Split Data"
      ],
      "metadata": {
        "id": "gdon0jzUIE8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n"
      ],
      "metadata": {
        "id": "KnI_sio9IClW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Define Feature Transformers and Model"
      ],
      "metadata": {
        "id": "HBA7gmknIIW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"clean_tokens\", outputCol=\"raw_features\", numFeatures=10000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20)\n"
      ],
      "metadata": {
        "id": "CqsqBkYhIGpg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Build and Train the Pipeline"
      ],
      "metadata": {
        "id": "y6usNgTbILqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[hashingTF, idf, lr])\n",
        "model = pipeline.fit(train_data)\n"
      ],
      "metadata": {
        "id": "dYt9Og4HIJyi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Evaluate Model"
      ],
      "metadata": {
        "id": "JElCsP2dIPu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "predictions = model.transform(test_data)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJXfFJddINuT",
        "outputId": "23447237-8697-4ac9-825b-82ae051559ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 83.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save evaluation metrics to JSON\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import json\n",
        "\n",
        "# Get metrics\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
        "metrics = {\n",
        "    \"accuracy\": evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}),\n",
        "    \"f1\": evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}),\n",
        "    \"precision\": evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"}),\n",
        "    \"recall\": evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"}),\n",
        "}\n",
        "\n",
        "# Save as JSON\n",
        "with open(\"/content/model_metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics, f, indent=4)\n"
      ],
      "metadata": {
        "id": "N0mqodEb39sR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4: Insights & Visualization"
      ],
      "metadata": {
        "id": "oF72E4xxOYBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Cleaned Review Data"
      ],
      "metadata": {
        "id": "M7vJ8sZ-Obsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(\"/content/module1_parquet_output\")\n",
        "df.select(\"Score\", \"Text\").show(3, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuQM-6UZIRRv",
        "outputId": "797871ad-dd84-472e-9c25-4b2b5480015d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Score|Text                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|5    |This will be the bottle that you grab from the fridge if you are really thirsty!  The bottles are big!  I like that about them because I can grab one and drink from it for a couple of hours and really stay hydrated.  The taste is just cool, clean and refreshing.  No aftertaste at all.  This is good water.                                                                                                   |\n",
            "|5    |I drank this on ice after a workout. It was very crisp, cool, and refreshing! I am normally not a bottled water fan, they all seem to taste like the plastic bottle to me. But this product had a distinctive sharp CLEAN taste, like it was extra filtered or something. It tastes better than the filtered water coming out of the dispenser in my fridge.<br /><br />I would buy this!!! DELICIOUS and refreshing!|\n",
            "|5    |Ordered 4 boxes. 4 different flavors. Have had the same issues with grounds and explosion as the other posters with all 4 boxes. Very disappointed.<br /><br />Brooklyn Bean replaced all boxes immediately.  Have had no problem with explosions.  Coffee is good too.  Customer service is top notch.                                                                                                              |\n",
            "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Clean, Tokenize, and Prepare Words"
      ],
      "metadata": {
        "id": "h8Ln5-lgOe2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower, regexp_replace, array_distinct\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "df = df.withColumn(\"clean_text\", lower(col(\"Text\")))\n",
        "df = df.withColumn(\"clean_text\", regexp_replace(\"clean_text\", \"[^a-zA-Z\\s]\", \"\"))\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words_raw\")\n",
        "tokenized = tokenizer.transform(df)\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"words_raw\", outputCol=\"filtered_words_temp\")\n",
        "tokenized = remover.transform(tokenized)\n",
        "\n",
        "# Remove duplicates inside each transaction (important for FP-Growth)\n",
        "tokenized = tokenized.withColumn(\"filtered_words\", array_distinct(col(\"filtered_words_temp\")))\n"
      ],
      "metadata": {
        "id": "XZrGW_JCOdMA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, size, explode, array, lit\n",
        "from itertools import combinations\n",
        "\n",
        "# Step 1: Get co-occurring pairs from filtered tokens\n",
        "token_pairs = tokenized.select(\"filtered_words\") \\\n",
        "    .filter(size(\"filtered_words\") > 1) \\\n",
        "    .rdd.flatMap(lambda row: [tuple(sorted(pair)) for pair in combinations(set(row[\"filtered_words\"]), 2)])\n",
        "\n",
        "# Step 2: Count occurrences of pairs\n",
        "pair_counts = token_pairs.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
        "pair_counts_df = pair_counts.map(lambda x: (x[0][0], x[0][1], x[1])).toDF([\"word1\", \"word2\", \"intersection\"])\n",
        "\n",
        "# Step 3: Count individual word occurrences\n",
        "word_counts = tokenized.select(explode(\"filtered_words\").alias(\"word\")) \\\n",
        "    .groupBy(\"word\").count().withColumnRenamed(\"count\", \"word_count\")\n",
        "\n",
        "word_counts1 = word_counts.withColumnRenamed(\"word\", \"word1\").withColumnRenamed(\"word_count\", \"count1\")\n",
        "word_counts2 = word_counts.withColumnRenamed(\"word\", \"word2\").withColumnRenamed(\"word_count\", \"count2\")\n",
        "\n",
        "# Step 4: Join and calculate Jaccard\n",
        "jaccard_df = pair_counts_df \\\n",
        "    .join(word_counts1, on=\"word1\") \\\n",
        "    .join(word_counts2, on=\"word2\") \\\n",
        "    .withColumn(\"jaccard\", col(\"intersection\") / (col(\"count1\") + col(\"count2\") - col(\"intersection\")))\n",
        "\n",
        "# Step 5: Save top 100 pairs to CSV for Streamlit\n",
        "jaccard_top = jaccard_df.orderBy(col(\"jaccard\").desc()).limit(100)\n",
        "jaccard_top.toPandas().to_csv(\"jaccard_similarity_top.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "sH1dit6TFOCc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Apply FP-Growth for Frequent Patterns"
      ],
      "metadata": {
        "id": "7myqQLWwOny8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "\n",
        "fp_growth = FPGrowth(itemsCol=\"filtered_words\", minSupport=0.02, minConfidence=0.5)\n",
        "fp_model = fp_growth.fit(tokenized)\n",
        "\n",
        "# sampled_data = tokenized.sample(fraction=0.5, seed=42)  # use 10% of data\n",
        "# fp_model = fp_growth.fit(sampled_data)\n",
        "\n",
        "\n",
        "# Show frequent itemsets\n",
        "fp_model.freqItemsets.orderBy(\"freq\", ascending=False)\n",
        "\n",
        "# Show association rules\n",
        "fp_model.associationRules.orderBy(\"confidence\", ascending=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLB_4RwDOgSV",
        "outputId": "8f82b805-fb21-4a2d-ee22-8e5972892e1a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[antecedent: array<string>, consequent: array<string>, confidence: double, lift: double, support: double]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data for Graph-Based Analysis (Optional Advanced Step)"
      ],
      "metadata": {
        "id": "aaXr9bXEOw7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col, monotonically_increasing_id, array_distinct\n",
        "\n",
        "# Step 1: Add unique row_id\n",
        "tokenized_with_id = tokenized.withColumn(\"row_id\", monotonically_increasing_id())\n",
        "\n",
        "# Step 2: Explode with row_id to track origin\n",
        "exploded = tokenized_with_id.select(\"row_id\", explode(\"filtered_words\").alias(\"word\"))\n",
        "\n",
        "# Step 3: Join on same row_id to create word pairs (excluding self-pairs)\n",
        "word_pairs = exploded.alias(\"a\").join(\n",
        "    exploded.alias(\"b\"),\n",
        "    (col(\"a.row_id\") == col(\"b.row_id\")) & (col(\"a.word\") < col(\"b.word\"))  # avoid a=b and duplicates\n",
        ").select(\n",
        "    col(\"a.word\").alias(\"src\"),\n",
        "    col(\"b.word\").alias(\"dst\")\n",
        ").distinct()\n",
        "\n",
        "# Show some sample pairs\n",
        "# word_pairs.show(10, truncate=False)\n"
      ],
      "metadata": {
        "id": "r_joTyzoOpLm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save FP Results"
      ],
      "metadata": {
        "id": "ErJAV828O0VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "# Convert array columns to string for CSV compatibility\n",
        "freq_itemsets_df = fp_model.freqItemsets.withColumn(\"items\", concat_ws(\", \", \"items\"))\n",
        "assoc_rules_df = fp_model.associationRules \\\n",
        "    .withColumn(\"antecedent\", concat_ws(\", \", \"antecedent\")) \\\n",
        "    .withColumn(\"consequent\", concat_ws(\", \", \"consequent\"))\n",
        "\n",
        "freq_itemsets_df = freq_itemsets_df.repartition(4)  # use 4 partitions\n",
        "assoc_rules_df = assoc_rules_df.repartition(4)\n",
        "\n",
        "# Save as CSV (multi-part for efficiency)\n",
        "freq_itemsets_df.repartition(10).write.option(\"header\", True).mode(\"overwrite\").csv(\"/content/freq_itemsets_csv\")\n",
        "assoc_rules_df.repartition(10).write.option(\"header\", True).mode(\"overwrite\").csv(\"/content/assoc_rules_csv\")\n"
      ],
      "metadata": {
        "id": "L-fYueRPOyeE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define output folder and final merged file name\n",
        "output_dir = \"/content/freq_itemsets_csv\"\n",
        "merged_file = \"/content/freq_itemsets_final.csv\"\n",
        "\n",
        "# List all part files\n",
        "part_files = [f for f in os.listdir(output_dir) if f.startswith(\"part-\") and f.endswith(\".csv\")]\n",
        "\n",
        "# Merge into one file\n",
        "with open(merged_file, 'w') as outfile:\n",
        "    for idx, fname in enumerate(sorted(part_files)):\n",
        "        with open(os.path.join(output_dir, fname), 'r') as infile:\n",
        "            if idx == 0:\n",
        "                shutil.copyfileobj(infile, outfile)  # keep header for the first file\n",
        "            else:\n",
        "                next(infile)  # skip header in remaining files\n",
        "                shutil.copyfileobj(infile, outfile)\n",
        "\n",
        "print(\"CSV saved at:\", merged_file)\n"
      ],
      "metadata": {
        "id": "xBWXAhbJO15N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363392f6-6d84-46a6-eba9-b8728f69f4d7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV saved at: /content/freq_itemsets_final.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define output folder and final merged file name\n",
        "output_dir = \"/content/assoc_rules_csv\"\n",
        "merged_file = \"/content/assoc_rules_csv_final.csv\"\n",
        "\n",
        "# List all part files\n",
        "part_files = [f for f in os.listdir(output_dir) if f.startswith(\"part-\") and f.endswith(\".csv\")]\n",
        "\n",
        "# Merge into one file\n",
        "with open(merged_file, 'w') as outfile:\n",
        "    for idx, fname in enumerate(sorted(part_files)):\n",
        "        with open(os.path.join(output_dir, fname), 'r') as infile:\n",
        "            if idx == 0:\n",
        "                shutil.copyfileobj(infile, outfile)  # keep header for the first file\n",
        "            else:\n",
        "                next(infile)  # skip header in remaining files\n",
        "                shutil.copyfileobj(infile, outfile)\n",
        "\n",
        "print(\"CSV saved at:\", merged_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfOrLm0RoRZl",
        "outputId": "66d4d9b5-0fe4-4935-d984-379e206e467e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV saved at: /content/assoc_rules_csv_final.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PageRank Implementation"
      ],
      "metadata": {
        "id": "nGqOxFO9MsH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Convert word pairs to Pandas DataFrame\n",
        "word_pairs_pd = word_pairs.toPandas()\n",
        "\n",
        "# Step 2: Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "G.add_edges_from(zip(word_pairs_pd['src'], word_pairs_pd['dst']))\n",
        "\n",
        "# Step 3: Run PageRank\n",
        "pagerank_scores = nx.pagerank(G, alpha=0.85)\n",
        "\n",
        "# Step 4: Convert to DataFrame and sort\n",
        "pagerank_df = pd.DataFrame(pagerank_scores.items(), columns=[\"word\", \"pagerank\"]).sort_values(by=\"pagerank\", ascending=False)\n",
        "\n",
        "# Step 5: Save top 100 ranked words for Streamlit\n",
        "pagerank_df.head(100).to_csv(\"/content/pagerank_words.csv\", index=False)\n",
        "print(\"PageRank CSV saved at: /content/pagerank_words.csv\")\n"
      ],
      "metadata": {
        "id": "eYTXcERYojSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQjLlZ5esvbT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}